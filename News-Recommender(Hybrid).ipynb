{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#File_path = '/home/linu/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "no_of_recommends = 20\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topic_Modeller(LDA_matrix):\n",
    "    \n",
    "    total_WordVocab = []\n",
    "    for i in range(0,len(cleaned_articles)) :\n",
    "        word_tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "        for words in word_tokens :\n",
    "            total_WordVocab.append(words)\n",
    "        counts = Counter(total_WordVocab)\n",
    "\n",
    "    vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "    stops_removed = [word for word in vocab.keys() if word not in stops]\n",
    "\n",
    "    Final_VocabDict = {j:i for i,j in enumerate(stops_removed)}\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(min_df=1,vocabulary=Final_VocabDict)\n",
    "\n",
    "\n",
    "    Tfidf_Matrix = Tfidf.fit_transform(cleaned_articles)\n",
    "\n",
    "    Lda = LatentDirichletAllocation(n_components=n_topics,max_iter=1,random_state=0)\n",
    "    \n",
    "    Lda_articlemat = Lda.fit_transform(Tfidf_Matrix)\n",
    "\n",
    "    return Lda_articlemat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda_articlemat = Topic_Modeller(cleaned_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 10)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create user profiles on the base of articles read and time spent  for implementing content approach\n",
    "\n",
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    \n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "        \n",
    "        Topic_weights = (article_read[i]*user_interest_timevalue)   #Ldamatrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(Topic_weights)                                      \n",
    "\n",
    "    return sum(normalize(user_profile))                             \n",
    "\n",
    "\n",
    "userProfile_One = user_profiler([wordtokens_article[600],wordtokens_article[99],wordtokens_article[120]],\n",
    "                             [Lda_articlemat[600],Lda_articlemat[99],Lda_articlemat[120]],\n",
    "                             [120,60,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "userprofile_one =  np.array(userProfile_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06380809, 1.03563938, 0.06380688, 1.05161394, 0.06380264,\n",
       "       0.0638015 , 1.03360147, 0.06380235, 0.06380346, 0.06380215])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Content_Recommends_Calculator(user_profile,Lda_articlemat) :\n",
    "    \n",
    "    user_interested_articles = []\n",
    "    \n",
    "    contents_interest_score = []\n",
    "\n",
    "    user_preffered_articles = cosine_similarity(userprofile_one.reshape(1,-1),Lda_articlemat)\n",
    "    \n",
    "    top_articles = np.sort(user_preffered_articles).flatten()[::-1][:10]\n",
    "    \n",
    "    user_interested_articles.append(top_articles)\n",
    "    \n",
    "    content_interest_score = (user_interested_articles[0] * 0.4)\n",
    "    \n",
    "    return content_interest_score\n",
    "\n",
    "\n",
    "content_recommended = Content_Recommends_Calculator(userprofile_one,Lda_articlemat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33218085, 0.33066672, 0.3303857 , 0.33036246, 0.32982406,\n",
       "       0.32849261, 0.3283279 , 0.3282375 , 0.32813684, 0.32796289])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CF apporach\n",
    "existing_users = np.random.random_sample(size=(1000,10))   #we take n existing users   \n",
    "new_user = np.random.random_sample(size=(1,10))            #we take a single new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collaborative_Recommends_Calculator(existing_usr,new_usr) :\n",
    "    \n",
    "    collaborative_interest_score = [ ]\n",
    "    sorted_collaborative_interest = [ ]\n",
    "    \n",
    "    collaborative_interest_score = cosine_similarity(existing_users,new_user)\n",
    "    \n",
    "    sorted_collaborative_interest = np.argsort(collaborative_interest_score,axis=0)[::-1][:10]\n",
    "    \n",
    "    sorted_collaborative_indexes = existing_users[sorted_collaborative_interest]\n",
    "    \n",
    "    collab_interest = np.mean(sorted_collaborative_indexes.reshape(-1,10),axis=0)\n",
    "    \n",
    "    collaborative_interest_scores = collab_interest*0.3\n",
    "    \n",
    "    return collaborative_interest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_recommended = Collaborative_Recommends_Calculator(existing_users,new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1401302 , 0.51475294, 0.15017204, 0.43437819, 0.44470077,\n",
       "       0.24788337, 0.31189648, 0.27263806, 0.1999394 , 0.29468474])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborative_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trends(trending) :\n",
    "\n",
    "    trends = np.mean(existing_users,axis=0)\n",
    "    Trending_news = cosine_similarity(trends.reshape(1,10),Lda_articlemat)\n",
    "    top= np.sort(Trending_news)[::-1][0][:10]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trending_Articles = Trends(existing_users)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09690846, 0.09766251, 0.09811722, 0.09881548, 0.09903137,\n",
       "       0.09911932, 0.09926018, 0.09927689, 0.09928779, 0.09929183])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trending_Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30%content based,40% collab,30% from trends\n",
    "\n",
    "def Hybrid_Calculator():\n",
    "    \n",
    "    hybrid_interests = np.add(content_recommended,collaborative_recommended,Trending_Articles) \n",
    "    \n",
    "    similar_scores = cosine_similarity(hybrid_interests.reshape(1,10),Lda_articlemat)\n",
    "    \n",
    "    recommended_article_address =  np.argsort(similar_scores)[::-1]\n",
    "    \n",
    "    return recommended_article_address    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_recommend_indexes =  Hybrid_Calculator() #we get hyrid interest with our variations of 0.4 content based and 0.6 collab based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended-Articles :\n",
      "\n",
      "\n",
      "1552    List of flag bearers for nations in the Rio Ol...\n",
      "2149    Celebrity Rumours that Rocked the Internet in ...\n",
      "2356    Celebrities Weddings that Cost More than your ...\n",
      "737      Papanasam   Papanaasam  Movie Review Round-up...\n",
      "1742                   Asian Games 2014  Day 9 Highlights\n",
      "3925    Budget 2016  MPs want income tax exemption lim...\n",
      "4661    Uber Rape Shame  Short Cuts Being Preferred to...\n",
      "3978    Govt hopeful of implementing GST bill next yea...\n",
      "2100    Most-Searched Celebrities 2014  Sunny Leone Be...\n",
      "4538    Modi US Visit  Itinerary of Indian PM s trip t...\n",
      "2370    Islamic State   Iranian Hulk  will fight Isis ...\n",
      "2446    War on Terror Updates  Russia  kill 320 terror...\n",
      "2780    Parliament winter session  PM Modi to address ...\n",
      "4615    Tata Motors  Adani Ports  Axis Bank  L T lead ...\n",
      "2229    Is there any steam left in IndiGo owner shares...\n",
      "4545    Ex-Service Chiefs Urge PM to Implement OROP  2...\n",
      "2122    Run Kerala Run  Sachin Tendulkar  Mohanlal  Am...\n",
      "2952    J K  Amarnath Yatra resumes  25 000 pilgrims l...\n",
      "3917    Economic Survey 2015-16  FY2017 GDP growth rat...\n",
      "1530    Rio Olympics 2016 hockey  Rupinder and Raghuna...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for articles in hybrid_recommend_indexes :\n",
    "    \n",
    "    print('Recommended-Articles :')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(title[articles][:no_of_recommends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
