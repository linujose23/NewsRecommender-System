{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_of_recommends = 5\n",
    "n_topics = 8\n",
    "\n",
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article\n",
    "\n",
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab = { }\n",
    "\n",
    "article_vocab = enumerate(cleaned_articles)\n",
    "\n",
    "total_words = []\n",
    "\n",
    "for i in range(0, len(cleaned_articles)):\n",
    "    tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "\n",
    "    for w in tokens:\n",
    "        total_words.append(w)\n",
    "counts = Counter(total_words)\n",
    "\n",
    "vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "stops_removed = [i for i in vocab.keys() if i not in stops]\n",
    "\n",
    "final_vocab = {j:i for i,j in enumerate(stops_removed)}\n",
    "\n",
    "\n",
    "tf_idf = TfidfVectorizer(vocabulary=final_vocab,min_df=1)\n",
    "\n",
    "article_vocabulary_matrix = tf_idf.fit_transform(cleaned_articles)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,max_iter=1,random_state=0)\n",
    "\n",
    "Lda_articlemat = lda.fit_transform(article_vocabulary_matrix)\n",
    "\n",
    "wordtokens_article = [word.split() for word in cleaned_articles]   #we tokenize each word in our article to divide by word per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 227 325\n"
     ]
    }
   ],
   "source": [
    "print(len(wordtokens_article[0]),len(wordtokens_article[2]),len(wordtokens_article[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01364421, 0.01364359, 0.90449491, 0.01363855, 0.01363802,\n",
       "       0.0136523 , 0.01364298, 0.01364544])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.15273519  1.48287497  1.48230629 31.24012811  1.81714006  1.4823308\n",
      "  1.48262537  1.48392406]\n"
     ]
    }
   ],
   "source": [
    "# a function to create user profiles\n",
    "\n",
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    \n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "        \n",
    "        user_profile_generate = (article_read[i]*user_interest_timevalue)          #Ldamatrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(user_profile_generate)                                      \n",
    "\n",
    "    return sum(user_profile)                                \n",
    "\n",
    "\n",
    "userProfile_One = user_profiler([wordtokens_article[600],wordtokens_article[99],wordtokens_article[120]],\n",
    "                         [Lda_articlemat[600],Lda_articlemat[99],Lda_articlemat[120]],\n",
    "                         [120,60,30])\n",
    "\n",
    "userProfile_Two = user_profiler([wordtokens_article[900],wordtokens_article[500],wordtokens_article[3000]],\n",
    "                         [Lda_articlemat[900],Lda_articlemat[500],Lda_articlemat[3000]],\n",
    "                         [111,120,180])\n",
    "\n",
    "userProfile_Three = user_profiler([wordtokens_article[600],wordtokens_article[4830],wordtokens_article[390]],\n",
    "                           [Lda_articlemat[600],Lda_articlemat[4830],Lda_articlemat[390]],\n",
    "                           [200,120,100])\n",
    "\n",
    "\n",
    "userprofile_List = [userProfile_One,userProfile_Two,userProfile_Three]\n",
    "print(userProfile_One)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.15273519  1.48287497  1.48230629 31.24012811  1.81714006  1.4823308\n",
      "  1.48262537  1.48392406]\n"
     ]
    }
   ],
   "source": [
    "print(userProfile_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_profiles = Normalizer(csr_matrix(userprofile_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similiar(profile_list) :\n",
    "    n = []\n",
    "\n",
    "    for profiles in userprofile_List:\n",
    "\n",
    "        user_preffered_articles = cosine_similarity(profiles.reshape(1,-1),Lda_articlemat)\n",
    "        a = np.argsort(user_preffered_articles).flatten()[::-1][:no_of_recommends]\n",
    "\n",
    "        n.append(a)\n",
    "\n",
    "    return n\n",
    "\n",
    "similarityscore = similiar(normalized_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1556, 1823, 2160, 1798, 1510]),\n",
       " array([ 688, 1679,  893, 3125,   68]),\n",
       " array([4744, 1977,  284, 4353,  195])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarityscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news['Title'][similarityscore[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "1556    Another blow in Rio Olympics preparations  as ...\n",
      "1823    Saina Nehwal loses China Open final against Li...\n",
      "2160     I  Box Office Collection  Vikram s Flick Gros...\n",
      "1798    All England Open  Saina Nehwal clinches quarte...\n",
      "1510    India at Rio Day 2 wrap  Gymnast Dipa Karmakar...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "688      Adi Kapyare Kootamani   Dhyan Sreenivasan-Nam...\n",
      "1679    Rio Olympics  Can India s badminton star Saina...\n",
      "893     Samsung Galaxy On5  2016  appears on Geekbench...\n",
      "3125    Srinagar on alert  3 policemen killed in two m...\n",
      "68      Kapu agitation violence  Pawan Kalyan condemns...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "4744    Indian Defence Minister Clears Deck for Acquis...\n",
      "1977    ATP World Tour Finals results  Federer tops gr...\n",
      "284     Vedalam  Vedhalam  box office collection  Ajit...\n",
      "4353    BJP-RSS Meet  Patel Quota  OROP Row  Banglades...\n",
      "195     Tollywood 2015  Top 10  hit blockbuster  highe...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in similarityscore:\n",
    "    print('\\n')\n",
    "    print('Recommended Articles :')\n",
    "    print('\\n')\n",
    "    print(news['Title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tamil Nadu  2 students dead  over 40 injured after live wire falls on bus'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['Title'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 7, ..., 4, 1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Lda_articlemat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
