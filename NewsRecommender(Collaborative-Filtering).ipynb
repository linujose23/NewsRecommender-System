{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_of_recommends = 5\n",
    "n_topics = 8\n",
    "\n",
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article\n",
    "\n",
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab = { }\n",
    "\n",
    "article_vocab = enumerate(cleaned_articles)\n",
    "\n",
    "total_words = []\n",
    "\n",
    "for i in range(0, len(cleaned_articles)):\n",
    "    tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "\n",
    "    for w in tokens:\n",
    "        total_words.append(w)\n",
    "counts = Counter(total_words)\n",
    "\n",
    "vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "stops_removed = [i for i in vocab.keys() if i not in stops]\n",
    "\n",
    "final_vocab = {j:i for i,j in enumerate(stops_removed)}\n",
    "\n",
    "\n",
    "tf_idf = TfidfVectorizer(vocabulary=final_vocab,min_df=1)\n",
    "\n",
    "article_vocabulary = tf_idf.fit_transform(cleaned_articles)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,max_iter=1,random_state=0)\n",
    "\n",
    "Lda_articlemat = lda.fit_transform(article_vocabulary)\n",
    "\n",
    "wordtokens_article = [word.split() for word in cleaned_articles] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat.shape            #topic modelling 4831 docs and 8 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_users = np.random.random_sample(size=(10000,8))  #we take any number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = np.random.random_sample(size=(1,8))  #we take a user to recommend him according to collaborative filtering by using cosine similiarty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similarity_Score = cosine_similarity(existing_users,new_user) #now we get our similar user score from our existing users and new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity_Score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we pick top 5 similiar users out of existing users by reversing the Similarity_Score for top scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_similars_users = np.argsort(Similarity_Score,axis=0)[::-1][:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5680],\n",
       "       [5126],\n",
       "       [9204],\n",
       "       [ 855],\n",
       "       [4042]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similars   #here we get our top 5 similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users= existing_users[top_similars]  #picking our top 5 similar user profiles out of existing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.8714688 , 0.09943635, 0.81975511, 0.90659792, 0.20167064,\n",
       "         0.59874779, 0.13842781, 0.11568995]],\n",
       "\n",
       "       [[0.53116645, 0.15099813, 0.38266618, 0.48156379, 0.0501611 ,\n",
       "         0.22416667, 0.2408782 , 0.03423717]],\n",
       "\n",
       "       [[0.72216707, 0.27146747, 0.88831756, 0.89256429, 0.04440116,\n",
       "         0.38850883, 0.08461993, 0.30834674]],\n",
       "\n",
       "       [[0.82336259, 0.03933566, 0.87710602, 0.90660293, 0.1628694 ,\n",
       "         0.52365853, 0.12202507, 0.28610161]],\n",
       "\n",
       "       [[0.55431594, 0.00759144, 0.52462022, 0.95735912, 0.16856865,\n",
       "         0.17572198, 0.13916037, 0.23306737]]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_user_profile = np.mean(top_users,axis=0)   # we take mean to get all the average or all the existing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8), (4831, 8))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_user_profile.shape,Lda_articlemat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_articles = cosine_similarity(avg_user_profile,Lda_articlemat) #now we find our similar articles according to our avg u.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53361795],\n",
       "       [0.12308587],\n",
       "       [0.17435128],\n",
       "       ...,\n",
       "       [0.12501959],\n",
       "       [0.15635202],\n",
       "       [0.53543936]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_articles.reshape(4831,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "interested_articles = np.argsort(interested_articles)[::-1]  #we fetch the indexes of our top 5 similar articles according to user interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1102, 4211,  230, ...,  614, 4484, 1187]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interested_articles #so these are the indexes of our top preffered articles by cosine of new and existing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news['Title'][interested_articles[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended-Articles :\n",
      "\n",
      "\n",
      "1102    Lenovo K4 Note open sale goes live on Amazon I...\n",
      "4211    Modi Government May Save Rs 88 800 Crore This ...\n",
      "230      Pathemari  release  Check out complete theatr...\n",
      "3742    Narendra Modi Likely to Announce   70 000 Cror...\n",
      "1776    Euro 2016  Wilmots worried about De Bruyne and...\n",
      "331     US box office collection   Shankarabharanam  f...\n",
      "3821    Nigerian army repels attack by suspected Boko ...\n",
      "909     Update LG G2 with Android Marshmallow via Cyan...\n",
      "439     Pre-Release Business  Will Akhil Akkineni s De...\n",
      "2521    Al Qaeda Claims Responsibility for Charlie Heb...\n",
      "3414    Nagaland  18 Arrested over Mob Lynching of Rap...\n",
      "1069    Samsung Galaxy A5 Android Marshmallow update  ...\n",
      "2414    Donald Trump last man standing after Kasich qu...\n",
      "4260    Tension Continues in Assam Over Killing of Bus...\n",
      "3627    Ranbir Kapoor and Katrina Kaif end their relat...\n",
      "2749    NIA seeks help from Interpol in nabbing D-Comp...\n",
      "2339    Photos from besieged Fallujah reveal plight of...\n",
      "3673    After  Dangal  Aamir Khan to work with Hollywo...\n",
      "2838    Dawood Ibrahim Made Phone Calls from Pakistan ...\n",
      "699     Kerala box office  Prithviraj s  Ennu Ninte Mo...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in [interested_articles[0]] :\n",
    "    print('Recommended-Articles :')\n",
    "    print('\\n')\n",
    "    print(news['Title'][i][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.argsort(existing_users[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    Phillipines drug war  1 800 drug-related death...\n",
       "2    US  South Korea begin joint military drill ami...\n",
       "7    Dialogue crucial in finding permanent solution...\n",
       "0    14 dead after bus falls into canal in Telangan...\n",
       "3    Illegal construction in Bengaluru  Will my hou...\n",
       "4    Punjab Gau Rakshak Dal chief held for assaulti...\n",
       "6    Infosys shares likely to fall on Tuesday after...\n",
       "1    Pratibha Tiwari molested on busy road   Saath ...\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['Title'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
