{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_of_recommends = 20\n",
    "n_topics = 10\n",
    "no_of_clusters = 10\n",
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article\n",
    "\n",
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Topic_Modeller(LDA_matrix):\n",
    "    \n",
    "    total_WordVocab = []\n",
    "    for i in range(0,len(cleaned_articles)) :\n",
    "        word_tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "        for words in word_tokens :\n",
    "            total_WordVocab.append(words)\n",
    "        counts = Counter(total_WordVocab)\n",
    "\n",
    "    vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "    stops_removed = [word for word in vocab.keys() if word not in stops]\n",
    "\n",
    "    Final_VocabDict = {j:i for i,j in enumerate(stops_removed)}\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(min_df=1,vocabulary=Final_VocabDict)\n",
    "\n",
    "\n",
    "    Tfidf_Matrix = Tfidf.fit_transform(cleaned_articles)\n",
    "\n",
    "    Lda = LatentDirichletAllocation(n_components=n_topics,max_iter=1,random_state=0)\n",
    "    \n",
    "    Lda_articlemat = Lda.fit_transform(Tfidf_Matrix)\n",
    "\n",
    "    return Lda_articlemat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]  #for userprofiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('topic_modeled.pickle','wb') as tm :\n",
    "#     pk.dump(Lda_articlemat,tm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lda_articlemat = Topic_Modeller(cleaned_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topic_modeled.pickle','rb') as topics :\n",
    "    \n",
    "    Lda_articlemat = pk.load(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=no_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_articles_matrix = kmeans.fit_transform(Lda_articlemat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_articles_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01256278, 0.34295554, 0.01256291, 0.01256288, 0.5565409 ,\n",
       "       0.01256336, 0.01256272, 0.0125628 , 0.01256307, 0.01256303])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.03596345, 1.00621297, 1.0260601 , 1.03605366, 1.01270339,\n",
       "       1.0336135 , 1.03342534, 1.01845823, 0.39285391, 0.73123103])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_articles_matrix[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    \n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "        \n",
    "        user_profile_generate = (article_read[i]*user_interest_timevalue)   #clustered_articles_matrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(user_profile_generate)                                      \n",
    "\n",
    "    return sum(normalize(user_profile))                             \n",
    "\n",
    "\n",
    "userProfile_one = user_profiler([wordtokens_article[600],wordtokens_article[99],wordtokens_article[120]],\n",
    "                             [clustered_articles_matrix[600],clustered_articles_matrix[99],clustered_articles_matrix[120]],\n",
    "                             [120,60,30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userProfile_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Content_Recommends_Calculator(user_profile,clustered_articles_matrix) :\n",
    "    \n",
    "    user_interested_articles = []\n",
    "    \n",
    "    contents_interest_score = []\n",
    "\n",
    "    user_preffered_articles = cosine_similarity(userProfile_one.reshape(1,-1),clustered_articles_matrix)\n",
    "    \n",
    "    top_articles = np.sort(user_preffered_articles).flatten()[::-1][:10]\n",
    "    \n",
    "    user_interested_articles.append(top_articles)\n",
    "    \n",
    "    content_interest_score = (user_interested_articles[0] * 0.4)\n",
    "    \n",
    "    return content_interest_score\n",
    "\n",
    "\n",
    "content_recommended = Content_Recommends_Calculator(userProfile_one,clustered_articles_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_users = np.random.random_sample(size=(1000,10))   #we take n existing users   \n",
    "new_user = np.random.random_sample(size=(1,10))            #we take a single new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collaborative_Recommends_Calculator(existing_usr,new_usr) :\n",
    "    \n",
    "    collaborative_interest_score = [ ]\n",
    "    sorted_collaborative_interest = [ ]\n",
    "    \n",
    "    collaborative_interest_score = cosine_similarity(existing_users,new_user)\n",
    "    \n",
    "    sorted_collaborative_interest = np.argsort(collaborative_interest_score,axis=0)[::-1][:10]\n",
    "    \n",
    "    sorted_collaborative_indexes = existing_users[sorted_collaborative_interest]\n",
    "    \n",
    "    collab_interest = np.mean(sorted_collaborative_indexes.reshape(-1,10),axis=0)\n",
    "    \n",
    "    collaborative_interest_scores = collab_interest*0.6\n",
    "    \n",
    "    return collaborative_interest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_recommended = Collaborative_Recommends_Calculator(existing_users,new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35610965, 0.40705177, 0.13578598, 0.46559692, 0.4294026 ,\n",
       "       0.10251596, 0.3331978 , 0.46642481, 0.3524417 , 0.13980315])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_recommend_indexes =  Hybrid_Calculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trends(trending) :\n",
    "\n",
    "    trends = np.mean(existing_users,axis=0)\n",
    "    Trending_news = cosine_similarity(trends.reshape(1,10),clustered_articles_matrix)\n",
    "    top= np.sort(Trending_news)[::-1][0][:10]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trending_Articles = Trends(existing_users)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hybrid_Calculator():\n",
    "    \n",
    "    hybrid_interests = np.add(content_recommended,collab_recommended)\n",
    "    \n",
    "    similar_scores = cosine_similarity(hybrid_interests.reshape(1,10),clustered_articles_matrix)\n",
    "    \n",
    "    recommended_article_address =  np.argsort(similar_scores)[::-1]\n",
    "    \n",
    "    return recommended_article_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_recommend_indexes =  Hybrid_Calculator() #we get hyrid interest with our variations of 0.4 content based and 0.6 collab based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 7, ..., 8, 9, 9], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxx = np.argmax(clustered_articles_matrix,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 5, 3, ..., 9, 0, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended-Articles :\n",
      "\n",
      "\n",
      "347     Jayasurya s Kuruthakedinte Koodane song from P...\n",
      "386      Pathemari   5 reasons why Mammootty-Jewel Mar...\n",
      "1908    Deepika  Salman  Katrina and Other Stars Who E...\n",
      "129        Kathakali movie review  Live audience response\n",
      "4601     Vedalam   Vedhalam  box office collection  Aj...\n",
      "554      Abhishek Bachchan s role in Housefull 3 revealed\n",
      "42      Vijay 61  Atlee being rewarded even before the...\n",
      "2035    Maharashtra Imposes Ban on Beef  Traders Upset...\n",
      "3196    Akshay Kumar in  Holiday 2   will the  Airlift...\n",
      "130            Gethu movie review  Live audience response\n",
      "1153    Lenovo A6000 4G Android Smartphone  First Impr...\n",
      "472      Kyaa Kool Hain Hum 3  banned in Pakistan for ...\n",
      "246      Baahubali 2  update  Anushka Shetty to take c...\n",
      "1869    Roundup   My Choice    Tribute to Paul Walker ...\n",
      "368     Rajamouli s  Baahubali   Mahesh s  Srimanthudu...\n",
      "134     Madonna Sebastian aka Celine of Premam to star...\n",
      "2971    Alia Bhatt and Katrina Kaif are the new BFF of...\n",
      "2034    Beef Ban in Maharashra  Raveena Tandon  Nimrat...\n",
      "2099    Farhan Akhtar  Priyanka Chopra Turn Singers fo...\n",
      "2195    5 20 aviation rule  AirAsia CEO takes a dig at...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for articles in hybrid_recommend_indexes :\n",
    "    \n",
    "    print('Recommended-Articles :')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(title[articles][:no_of_recommends])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
