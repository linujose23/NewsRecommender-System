{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "path = 'C:/Users/LJ/ML codes/'\n",
    "news = pd.read_csv(path + 'news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "no_of_recommends = 30\n",
    "n_topics = 4\n",
    "no_of_clusters = 10\n",
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle as pk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article\n",
    "\n",
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Topic_Modeller(LDA_matrix):\n",
    "    \n",
    "    total_WordVocab = []\n",
    "    for i in range(0,len(cleaned_articles)) :\n",
    "        word_tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "        for words in word_tokens :\n",
    "            total_WordVocab.append(words)\n",
    "        counts = Counter(total_WordVocab)\n",
    "\n",
    "    vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "    stops_removed = [word for word in vocab.keys() if word not in stops]\n",
    "\n",
    "    Final_VocabDict = {j:i for i,j in enumerate(stops_removed)}\n",
    "    \n",
    "    \n",
    "    Vectorizer = CountVectorizer(cleaned_articles,vocabulary=Final_VocabDict)\n",
    "    \n",
    "    Vectorized_Matrix = Vectorizer.fit_transform(cleaned_articles)\n",
    "\n",
    "    \n",
    "    Tfidf_Matrix = Tfidf.fit_transform(cleaned_articles)\n",
    "\n",
    "    Lda = LatentDirichletAllocation(learning_decay=0.9,n_components=n_topics,max_iter=1,random_state=0)\n",
    "    \n",
    "    Lda_articlemat = Lda.fit_transform(Vectorized_Matrix)\n",
    "\n",
    "    return  Lda_articlemat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]  #for userprofiles to calculate average time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('topic_modeled.pickle','rb') as topics :\n",
    "    \n",
    "#     Lda_articlemat = pk.load(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with open('articles_matrix.pickle','rb') as c_mat :   #pickled the clustered output for fast processing\n",
    "    \n",
    "#      articles_matrix = pk.load(c_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD = GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = LatentDirichletAllocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix = Topic_Modeller(cleaned_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_components': [3, 4, 5, 6, 7, 8, 9, 10], 'learning_decay': [0.2, 0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [3,4,5,6,7,8,9,10], 'learning_decay': [.2,.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "\n",
    "# Init Grid Search Class\n",
    "GD = GridSearchCV(Lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "GD.fit(vector)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = GD.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.9, 'n_components': 4}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD.best_params_               #4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans =  KMeans(n_clusters=4,random_state=101,max_iter=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_articles = []\n",
    "for i in range(5):\n",
    "    recommended_articles.append(title[labels==i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2     US  South Korea begin joint military drill ami...\n",
       " 4     Punjab Gau Rakshak Dal chief held for assaulti...\n",
       " 8     School bus overturns in Jammu killing 1 and in...\n",
       " 12    Tamil Nadu  2 students dead  over 40 injured a...\n",
       " 14    42 missing in Mahad bridge collapse  Met says ...\n",
       " Name: Title, dtype: object,\n",
       " 1     Pratibha Tiwari molested on busy road   Saath ...\n",
       " 5     Phillipines drug war  1 800 drug-related death...\n",
       " 7     Dialogue crucial in finding permanent solution...\n",
       " 10    Karnataka bus accident  8 schoolchildren dead ...\n",
       " 11    Meghalaya accident  At least 30 killed and 8 i...\n",
       " Name: Title, dtype: object,\n",
       " 0     14 dead after bus falls into canal in Telangan...\n",
       " 3     Illegal construction in Bengaluru  Will my hou...\n",
       " 6     Infosys shares likely to fall on Tuesday after...\n",
       " 9     Rajasthan  Villagers rescue 50 kids after scho...\n",
       " 42    Vijay 61  Atlee being rewarded even before the...\n",
       " Name: Title, dtype: object,\n",
       " 28    Multilingual actress Pranitha escapes road acc...\n",
       " 30    Box office collection   Jil Jung Juk    Deadpo...\n",
       " 31    Ajith set to take up Vishnuvardhan s historica...\n",
       " 32     Theri  teaser  Will Vijay starrer clip reach ...\n",
       " 35     Jil Jung Juk  movie review  Live audience res...\n",
       " Name: Title, dtype: object,\n",
       " Series([], Name: Title, dtype: object)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
