{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#File_path = '/home/linu/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "no_of_recommends = 20\n",
    "n_topics = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topic_Modeller(LDA_matrix):\n",
    "    \n",
    "    total_WordVocab = []\n",
    "    for i in range(0,len(cleaned_articles)) :\n",
    "        word_tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "        for words in word_tokens :\n",
    "            total_WordVocab.append(words)\n",
    "        counts = Counter(total_WordVocab)\n",
    "\n",
    "    vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "    stops_removed = [word for word in vocab.keys() if word not in stops]\n",
    "\n",
    "    Final_VocabDict = {j:i for i,j in enumerate(stops_removed)}\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(min_df=1,vocabulary=Final_VocabDict)\n",
    "\n",
    "\n",
    "    Tfidf_Matrix = Tfidf.fit_transform(cleaned_articles)\n",
    "\n",
    "    Lda = LatentDirichletAllocation(n_components=10,max_iter=1,random_state=0)\n",
    "    \n",
    "    Lda_articlemat = Lda.fit_transform(Tfidf_Matrix)\n",
    "\n",
    "    return Lda_articlemat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda_articlemat = Topic_Modeller(cleaned_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create user profiles on the base of articles read and time spent  for implementing content approach\n",
    "\n",
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    \n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "        \n",
    "        user_profile_generate = (article_read[i]*user_interest_timevalue)   #Ldamatrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(user_profile_generate)                                      \n",
    "\n",
    "    return sum(normalize(user_profile))                             \n",
    "\n",
    "\n",
    "userProfile_One = user_profiler([wordtokens_article[600],wordtokens_article[99],wordtokens_article[120]],\n",
    "                             [Lda_articlemat[600],Lda_articlemat[99],Lda_articlemat[120]],\n",
    "                             [120,60,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "userprofile_one =  np.array(userProfile_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06380809, 1.03563938, 0.06380688, 1.05161394, 0.06380264,\n",
       "       0.0638015 , 1.03360147, 0.06380235, 0.06380346, 0.06380215])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Content_Recommends_Calculator(user_profile,Lda_articlemat) :\n",
    "    \n",
    "    user_interested_articles = []\n",
    "    \n",
    "    contents_interest_score = []\n",
    "\n",
    "    user_preffered_articles = cosine_similarity(userprofile_one.reshape(1,-1),Lda_articlemat)\n",
    "    \n",
    "    top_articles = np.sort(user_preffered_articles).flatten()[::-1][:10]\n",
    "    \n",
    "    user_interested_articles.append(top_articles)\n",
    "    \n",
    "    content_interest_score = (user_interested_articles[0] * 0.4)\n",
    "    \n",
    "    return content_interest_score\n",
    "\n",
    "\n",
    "content_recommended = Content_Recommends_Calculator(userprofile_one,Lda_articlemat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33218085, 0.33066672, 0.3303857 , 0.33036246, 0.32982406,\n",
       "       0.32849261, 0.3283279 , 0.3282375 , 0.32813684, 0.32796289])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CF apporach\n",
    "existing_users = np.random.random_sample(size=(1000,10))   #we take n existing users   \n",
    "new_user = np.random.random_sample(size=(1,10))            #we take a single new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collaborative_Recommends_Calculator(existing_usr,new_usr) :\n",
    "    \n",
    "    collaborative_interest_score = [ ]\n",
    "    sorted_collaborative_interest = [ ]\n",
    "    \n",
    "    collaborative_interest_score = cosine_similarity(existing_users,new_user)\n",
    "    \n",
    "    sorted_collaborative_interest = np.argsort(collaborative_interest_score,axis=0)[::-1][:10]\n",
    "    \n",
    "    sorted_collaborative_indexes = existing_users[sorted_collaborative_interest]\n",
    "    \n",
    "    collab_interest = np.mean(sorted_collaborative_indexes.reshape(-1,10),axis=0)\n",
    "    \n",
    "    collaborative_interest_scores = collab_interest*0.6\n",
    "    \n",
    "    return collaborative_interest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_recommended = Collaborative_Recommends_Calculator(existing_users,new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42538367, 0.12472748, 0.27917413, 0.52935857, 0.36303798,\n",
       "       0.48767273, 0.50683032, 0.344337  , 0.17345567, 0.23187774])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hybrid_Calculator():\n",
    "    \n",
    "    hybrid_interests = np.add(content_recommended,collab_recommended)\n",
    "    \n",
    "    similar_scores = cosine_similarity(hybrid_interests.reshape(1,10),Lda_articlemat)\n",
    "    \n",
    "    recommended_article_address =  np.argsort(similar_scores)[::-1]\n",
    "    \n",
    "    return recommended_article_address    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_recommend_indexes =  Hybrid_Calculator() #we get hyrid interest with our variations of 0.4 content based and 0.6 collab based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended-Articles :\n",
      "\n",
      "\n",
      "1813    2016 Premier Badminton League  Complete team s...\n",
      "4495    Asian Shares Recover from Three-year Lows Whil...\n",
      "383      Oru Vadakkan Selfie  Movie Review  A Hilariou...\n",
      "1331    Apple CEO s likely agenda for multi-day India ...\n",
      "3606    Shah Rukh Khan Beats Salman to Win  Sexiest Kh...\n",
      "1937     OK Kanmani  Trailer  Train  Rain  Beach  Roma...\n",
      "4115    Modi s favourability rating rises to 87   jobs...\n",
      "973                          A case for twin-lens cameras\n",
      "241     Rajamma at Yahoo review  Predictable  simple  ...\n",
      "2640     Captain America  Civil War  spoilers  Spider-...\n",
      "954     Update OnePlus One to Android Marshmallow via ...\n",
      "2154     Ormayundo Ee Mugham  Critics Review Roundup  ...\n",
      "3351    Opposition Unites to Demand Sushma Swaraj s Re...\n",
      "2518    France police raid homes  vow it s  just the b...\n",
      "4407    Start-up India  Japan s SoftBank to invest  10...\n",
      "1599    India at Rio Olympics  Shiva Thapa  Manoj Kuma...\n",
      "2130    Mohanlal Offers to Return National Games Perfo...\n",
      "4742    India Ramping up Submarine Arsenal  Foreign Co...\n",
      "2042    Salman Jokes about  Smooching  Shah Rukh  Says...\n",
      "1207    Update Google Nexus 7  2012  with Android Mars...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for articles in hybrid_recommend_indexes :\n",
    "    \n",
    "    print('Recommended-Articles :')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(title[articles][:no_of_recommends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
